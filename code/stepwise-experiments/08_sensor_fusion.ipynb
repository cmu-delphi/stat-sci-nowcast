{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a25b5a3",
   "metadata": {},
   "source": [
    "# Sensor fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebf75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import date_range\n",
    "from sklearn.linear_model import Lasso\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# first party\n",
    "from data_containers import GroundTruth, LocationSeries, SensorConfig, Sensors\n",
    "from fusion import covariance, fusion\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c0572",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Directory:\n",
    "    ROOT = '../data/'\n",
    "    \n",
    "    def __init__(self, gt_indicator = Config.ground_truth_indicator):\n",
    "        self.gt = gt_indicator\n",
    "        self.infections_root_dir = './results/ntf_tapered/'\n",
    "        self.indicator_root_dir = os.path.join(Directory.ROOT, 'indicators')\n",
    "        self.sensor_root_dir = os.path.join(Directory.ROOT, 'sensors')\n",
    "        self.jhu_path = os.path.join(\n",
    "            Directory.ROOT, f'jhu-csse_confirmed_incidence_prop/{self.gt.source}_{self.gt.signal}')\n",
    "       \n",
    "    def deconv_gt_file(self, as_of):\n",
    "        return os.path.join(self.infections_root_dir, f'as_of_{as_of}.p')\n",
    "    \n",
    "    def indicator_file(self, indicator, as_of):\n",
    "        return os.path.join(\n",
    "            self.indicator_root_dir,\n",
    "            f'{indicator.source}-{indicator.signal}_{as_of}.p')\n",
    "    \n",
    "    def sensor_file(self, config, as_of):\n",
    "        return os.path.join(\n",
    "            self.sensor_root_dir,\n",
    "            f'{config.source}_{config.signal}_{as_of}.p')\n",
    "    \n",
    "    def maybe_load_file(self, file_name, verbose=False):\n",
    "        if not os.path.isfile(file_name):\n",
    "            if verbose:\n",
    "                print(file_name, 'does not exist')\n",
    "            return False\n",
    "        \n",
    "        return pickle.load(open(file_name, 'rb'))\n",
    "    \n",
    "    def maybe_write_file(self, data, file_name, overwrite=False, verbose=False):\n",
    "        if os.path.isfile(file_name) and not overwrite:\n",
    "            if verbose:\n",
    "                print(file_name, 'exists')\n",
    "            return False\n",
    "        \n",
    "        dir_name = os.path.dirname(file_name)\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.makedirs(dir_name)   \n",
    "        \n",
    "        pickle.dump(data, open(file_name, 'wb'))\n",
    "        return True\n",
    "    \n",
    "    @staticmethod\n",
    "    def exists(file_name, overwrite=False):\n",
    "        if os.path.isfile(file_name) and not overwrite:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "def conform(location_series):\n",
    "    if location_series is None:\n",
    "        return None\n",
    "    \n",
    "    if location_series.data is None or np.isnan(location_series.values).all():\n",
    "        return None\n",
    "    \n",
    "    if isinstance(location_series.dates[0], datetime):\n",
    "        location_series.data = dict(zip([d.date() for d in location_series.dates],\n",
    "                                        location_series.values))\n",
    "    return location_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up all the method configurations.\n",
    "# The 'fast_' versions use a subset of lower-latency sensors\n",
    "infections_config = SensorConfig('jhu-csse', 'confirmed_incidence_prop','deconv_infections', 2)\n",
    "ar3_config = SensorConfig('ar3', 'ntf_tapered_infections', 'ar3', lag=1)\n",
    "simple_avg_config = SensorConfig('all', 'simple_average', 'average', 4)\n",
    "fast_simple_avg_config = SensorConfig('fast_all', 'simple_average', 'average', 1)\n",
    "simple_avg_no_google_aa_config = SensorConfig('all_no_google_aa', 'simple_average', 'average', 4)\n",
    "fast_simple_avg_no_google_aa_config= SensorConfig('fast_all_no_google_aa', 'simple_average', 'average', 1)\n",
    "\n",
    "simple_reg_config = SensorConfig('all', 'simple_reg', 'regression', 4)\n",
    "fast_simple_reg_config = SensorConfig('fast_all', 'simple_reg', 'regression', 1)\n",
    "\n",
    "ridge_config = SensorConfig('all', 'ridge', 'ridge', 4)\n",
    "fast_ridge_config = SensorConfig('fast_all', 'ridge', 'ridge', 1)\n",
    "\n",
    "lasso_config = SensorConfig('all', 'lasso', 'lasso', 4)\n",
    "fast_lasso_config = SensorConfig('fast_all', 'lasso', 'lasso', 1)\n",
    "\n",
    "kf_sf_config = SensorConfig('all', 'kf_sf', 'kf_sf', 4)\n",
    "fast_kf_sf_config = SensorConfig('fast_all', 'kf_sf', 'kf_sf', 1)\n",
    "\n",
    "num_backcast = 15\n",
    "directory = Directory(infections_config)\n",
    "states = sorted(Config.states - set(['dc']))\n",
    "geos = sorted((set(states)| Config.megacounties | Config.top_counties) - set(['11000', '11001']))\n",
    "evaluation_geos = sorted((set(states) | Config.top_counties) - set(['11001']))\n",
    "get_geo_type = lambda x: 'county' if x.isnumeric() else 'state'\n",
    "state_map = {}\n",
    "state_type_map = {}\n",
    "for state in states:\n",
    "    fips_code = Config.state_fips[state]\n",
    "    state_map[state] = [geo for geo in geos if geo == state or geo[:2] == fips_code] \n",
    "    state_type_map[state] = [(geo, get_geo_type(geo)) for geo in state_map[state]]\n",
    "\n",
    "as_of_range = Config.as_of_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44e68d",
   "metadata": {},
   "source": [
    "## Load infections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2e7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "infections_data = GroundTruth()\n",
    "for as_of in tqdm(as_of_range):\n",
    "    infections_file = directory.deconv_gt_file(as_of)\n",
    "    data = directory.maybe_load_file(infections_file)\n",
    "    assert data is not None, as_of\n",
    "\n",
    "    for k in data.keys():\n",
    "        vals = conform(data[k])\n",
    "        if vals is None:\n",
    "            continue\n",
    "        infections_data.add_data(as_of, k, vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a92a5",
   "metadata": {},
   "source": [
    "## Load sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sensors(as_of_range, sensor_configs, directory):\n",
    "    sensors = Sensors()\n",
    "    for as_of in tqdm(as_of_range):\n",
    "        for sensor, config in sensor_configs.items():\n",
    "            sensor_file = directory.sensor_file(config, as_of)\n",
    "            sensor_data = directory.maybe_load_file(sensor_file)\n",
    "            if not sensor_data:\n",
    "                continue\n",
    "            for k in sensor_data.keys():\n",
    "                vals = conform(sensor_data[k])\n",
    "                if vals is None:\n",
    "                    continue\n",
    "                sensors.add_data(as_of, sensor, sensor_data[k].geo_value, vals)\n",
    "    return sensors\n",
    "\n",
    "full_sensors = {\n",
    "    'fb_cliic': Config.fb_cliic,\n",
    "    'dv_cli': Config.dv_cli,\n",
    "    'google_aa': Config.google_aa,\n",
    "    'chng_cli': Config.chng_cli,\n",
    "    'chng_covid': Config.chng_covid,\n",
    "    'ar3': ar3_config,\n",
    "}\n",
    "\n",
    "fast_sensors = {\n",
    "    'fb_cliic': Config.fb_cliic,\n",
    "    'google_aa': Config.google_aa,\n",
    "    'ar3': ar3_config,\n",
    "}\n",
    "\n",
    "full_sensor_data = load_sensors(as_of_range, full_sensors, directory)\n",
    "fast_sensor_data = load_sensors(as_of_range, fast_sensors, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2889896",
   "metadata": {},
   "source": [
    "## Simple average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_simple_average(output_config, as_of_range, sensor_dict, sensor_data, directory, overwrite):\n",
    "    for as_of in tqdm(as_of_range):\n",
    "        output_file = directory.sensor_file(output_config, as_of)\n",
    "        last_date = as_of - timedelta(output_config.lag)\n",
    "        full_dates = [d.date() for d in date_range(\n",
    "            as_of - timedelta(num_backcast), last_date)]\n",
    "\n",
    "        if directory.exists(output_file, overwrite):\n",
    "            print(output_file, 'exists')\n",
    "            continue\n",
    "\n",
    "        output = {}\n",
    "        for geo in evaluation_geos:            \n",
    "            geo_output = []\n",
    "            for sensor in sensor_dict.keys():\n",
    "                signal = sensor_data.get_data(as_of, sensor, geo)\n",
    "                if signal is None:\n",
    "                    continue\n",
    "                try:\n",
    "                    vals = signal.get_data_range(full_dates[0], full_dates[-1])\n",
    "                    geo_output.append(vals)\n",
    "                except Exception as e:\n",
    "                    print(e, geo, as_of, sensor, 'failed')\n",
    "                    continue\n",
    "            output[geo] = LocationSeries(\n",
    "                geo, get_geo_type(geo), dict(zip(full_dates, np.nanmean(geo_output, axis=0)))\n",
    "            )\n",
    "            \n",
    "        directory.maybe_write_file(output, output_file, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ea1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_simple_average(simple_avg_config, as_of_range, full_sensors, full_sensor_data, directory, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf80810",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_simple_average(fast_simple_avg_config, as_of_range, fast_sensors, fast_sensor_data, directory, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f7f18b",
   "metadata": {},
   "source": [
    "Run without GOOGLE-AA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sensors_no_google_aa = dict((k,v) for k, v in full_sensors.items() if k != \"google_aa\")\n",
    "gen_simple_average(simple_avg_no_google_aa_config, as_of_range, full_sensors_no_google_aa, \n",
    "                   full_sensor_data, directory, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4660cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_sensors_no_google_aa = dict((k,v) for k, v in fast_sensors.items() if k != \"google_aa\")\n",
    "gen_simple_average(fast_simple_avg_no_google_aa_config, \n",
    "                   as_of_range, fast_sensors_no_google_aa,\n",
    "                   fast_sensor_data, directory, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4a2f7",
   "metadata": {},
   "source": [
    "## Simple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc57533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_simple_regression(output_config, as_of_range, sensor_dict, \n",
    "                          infections_data, sensor_data, directory, overwrite):\n",
    "    def _regression(y, X):\n",
    "        return np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "        \n",
    "    p = len(sensor_dict)\n",
    "    d = Config.max_delay_days\n",
    "    col_order = sorted(sensor_dict.keys())\n",
    "    for as_of in tqdm(as_of_range):\n",
    "        output_file = directory.sensor_file(output_config, as_of)\n",
    "        last_infection_date = as_of - timedelta(directory.gt.lag)\n",
    "        last_date = as_of - timedelta(output_config.lag)\n",
    "        last_training_date = min(last_infection_date, last_date)\n",
    "        training_dates = [d.date() for d in date_range(as_of - timedelta(2*d), last_training_date)]\n",
    "        n_train = len(training_dates)\n",
    "        n_test = (last_date - last_infection_date).days\n",
    "        \n",
    "        if directory.exists(output_file, overwrite):\n",
    "            print(output_file, 'exists')\n",
    "            continue\n",
    "\n",
    "        output = {}\n",
    "        for geo in evaluation_geos:            \n",
    "            response_series = infections_data.get_data(as_of, geo)\n",
    "            response = response_series.get_data_range(training_dates[0], training_dates[-1])\n",
    "            covariates = np.full((n_train, p), np.nan)\n",
    "            \n",
    "            test_covariates, test_dates = None, None\n",
    "            if n_test >= 0:\n",
    "                test_covariates = np.full((n_test, p), np.nan)\n",
    "                test_dates = [d.date() for d in date_range(\n",
    "                    last_date - timedelta(n_test), last_date)][1:]\n",
    "            \n",
    "            for j, col in enumerate(col_order):\n",
    "                covariate = sensor_data.get_data(as_of, col, geo)\n",
    "                if covariate is None: continue\n",
    "                try:\n",
    "                    covariates[:, j] = covariate.get_data_range(training_dates[0], training_dates[-1])\n",
    "                    if test_covariates is not None:\n",
    "                        test_covariates[:, j] = covariate.get_data_range(test_dates[0], test_dates[-1])\n",
    "                except Exception as e:\n",
    "                    print(e, geo, as_of, col, 'failed')\n",
    "                    continue\n",
    "            \n",
    "            nan_cols = np.all(np.isnan(covariates), axis=0)\n",
    "            if test_covariates is not None:\n",
    "                nan_cols = np.logical_or(nan_cols, np.any(np.isnan(test_covariates), axis=0))\n",
    "            covariates = np.c_[np.ones(covariates.shape[0]), covariates[:, ~nan_cols]]\n",
    "            try:\n",
    "                beta = _regression(response, covariates)\n",
    "            except np.linalg.LinAlgError as e:\n",
    "                if str(e) != \"Singular matrix\":\n",
    "                    raise\n",
    "                else:\n",
    "                    print(geo, as_of, 'Singular matrix')\n",
    "                    continue\n",
    "                    \n",
    "            est = (covariates @ beta).flatten()\n",
    "            dates = training_dates\n",
    "            if test_covariates is not None and test_dates is not None:\n",
    "                est = np.r_[est, \n",
    "                            (np.c_[np.ones(test_covariates.shape[0]), \n",
    "                                   test_covariates[:, ~nan_cols]] @ beta).flatten()]\n",
    "                dates = np.r_[dates, test_dates]\n",
    "    \n",
    "            est = est[-num_backcast:]\n",
    "            dates = dates[-num_backcast:]\n",
    "            output[geo] = LocationSeries(geo, get_geo_type(geo), dict(zip(dates, est)))   \n",
    "        directory.maybe_write_file(output, output_file, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fea2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_simple_regression(\n",
    "    simple_reg_config, as_of_range, full_sensors, infections_data, \n",
    "    full_sensor_data, directory, False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_simple_regression(\n",
    "    fast_simple_reg_config, as_of_range, fast_sensors, infections_data, \n",
    "    fast_sensor_data, directory, False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0682400",
   "metadata": {},
   "source": [
    "## Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e438c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lasso_regression(y, X, lam):\n",
    "    mod = Lasso(alpha=lam, fit_intercept=False)\n",
    "    mod.fit(X, np.array(y).reshape(-1,))\n",
    "    return mod.coef_\n",
    "\n",
    "def _ridge_regression(y, X, lam):\n",
    "    return np.linalg.inv(X.T @ X + lam*np.eye(X.shape[1])) @ X.T @ y\n",
    "\n",
    "def mean_impute(A, col_means=None):\n",
    "    miss_inds = np.where(np.isnan(A))\n",
    "    if col_means is not None:\n",
    "        A[miss_inds] = np.take(col_means, miss_inds[1])\n",
    "    else:\n",
    "        A[miss_inds] = np.take(np.nanmean(A, axis=0), miss_inds[1])\n",
    "    return A\n",
    "\n",
    "def response_matrix(as_of, dates, input_geos, response_data):\n",
    "    n, p = len(dates), len(input_geos)\n",
    "    matrix = np.full((n, p), np.nan)\n",
    "    for i, date in enumerate(dates):\n",
    "        for j, geo in enumerate(input_geos):\n",
    "            matrix[i, j] = response_data.get_val(as_of, date, geo)\n",
    "    return matrix\n",
    "    \n",
    "def covariate_matrix(as_of, dates, input_pairs, covariate_data,\n",
    "                     return_nan_cols=True):\n",
    "    n, p = len(dates), len(input_pairs)\n",
    "    matrix = np.full((n, p), np.nan)\n",
    "    for i, date in enumerate(dates):\n",
    "        for j, (covariate, geo) in enumerate(input_pairs):\n",
    "            matrix[i, j] = covariate_data.get_val(as_of, date, covariate, geo)   \n",
    "    if return_nan_cols:\n",
    "        nan_cols = np.all(np.isnan(matrix), axis=0)\n",
    "        return matrix, nan_cols\n",
    "    return matrix\n",
    "\n",
    "def gen_regularized_regression(\n",
    "    output_config, as_of_range, fit_func, sensor_dict, \n",
    "    infections_data, sensor_data,\n",
    "    cv_grid, cv_folds, directory, overwrite\n",
    "):\n",
    "    \n",
    "    d = Config.max_delay_days\n",
    "    input_sensors = sorted(sensor_dict.keys())\n",
    "    for as_of in tqdm(as_of_range[::-1]):\n",
    "        output_file = directory.sensor_file(output_config, as_of)\n",
    "        if directory.exists(output_file, overwrite):\n",
    "            print(output_file, 'exists')\n",
    "            continue\n",
    "            \n",
    "        # find last observed data according to given lags\n",
    "        last_infection_date = as_of - timedelta(directory.gt.lag)\n",
    "        last_date = as_of - timedelta(output_config.lag)\n",
    "        last_training_date = min(last_infection_date, last_date)\n",
    "        \n",
    "        # use 2d training window, determine if out-of-sample estimates can be made (n_test)\n",
    "        training_dates = [d.date() for d in date_range(as_of - timedelta(2*d), last_training_date)]\n",
    "        n_train = len(training_dates)\n",
    "        n_test = (last_date - last_infection_date).days\n",
    "        \n",
    "        output = {}\n",
    "        # train one model per state\n",
    "        for state in states:\n",
    "            input_locations = state_map[state]\n",
    "            input_pairs = np.array(list(itertools.product(input_sensors, input_locations)))\n",
    "            for geo in input_locations:\n",
    "                \n",
    "                # skip any geographies we are not interested in (megacounties)\n",
    "                if geo not in evaluation_geos:\n",
    "                    continue\n",
    "                \n",
    "                # pull training data, and find which inputs are available on this date\n",
    "                response_series = infections_data.get_data(as_of, geo)\n",
    "                response = response_series.get_data_range(training_dates[0], training_dates[-1])\n",
    "                covariates, nan_cols = covariate_matrix(as_of, training_dates, input_pairs, sensor_data)\n",
    "                test_covariates, test_dates = None, None\n",
    "                if n_test >= 0:\n",
    "                    test_dates = [d.date() for d in date_range(\n",
    "                        last_date - timedelta(n_test), last_date)][1:]\n",
    "                    test_covariates, test_nan_cols = covariate_matrix(\n",
    "                        as_of, test_dates, input_pairs, sensor_data\n",
    "                    )\n",
    "                    nan_cols = np.logical_or(nan_cols, test_nan_cols)\n",
    "                covariates = np.c_[np.ones(covariates.shape[0]), covariates[:, ~nan_cols]]\n",
    "                covariates = mean_impute(covariates)\n",
    "                \n",
    "                # perform cross-validation to find tuning parameters\n",
    "                cv_errors = np.full((len(cv_grid), cv_folds, num_backcast), np.inf)\n",
    "                for j, fold in enumerate(range(1, cv_folds + 1)):\n",
    "                    for i, lam in enumerate(cv_grid):\n",
    "                        n = covariates.shape[0] - fold\n",
    "                        if n_test >= 0: # produce a 1-ahead prediction\n",
    "                            n -= 1\n",
    "                            cv_test_covariates = covariates[n]\n",
    "                            cv_test_response = response[n]\n",
    "\n",
    "                        cv_response = response[:n]\n",
    "                        cv_covariates = covariates[:n]\n",
    "                        try:\n",
    "                            beta = fit_func(cv_response, cv_covariates, lam)\n",
    "\n",
    "                            cv_predictions = (cv_covariates @ beta).flatten()\n",
    "                            assert len(cv_predictions) == n\n",
    "                            if n_test >= 0:\n",
    "                                cv_predictions = np.r_[cv_predictions,\n",
    "                                                       cv_test_covariates @ beta]\n",
    "                                cv_response = np.r_[cv_response, cv_test_response]\n",
    "\n",
    "                            cv_errors[i, j, :] = np.abs(cv_predictions - cv_response)[-num_backcast:]\n",
    "                        except np.linalg.LinAlgError as e:\n",
    "                            print(geo, as_of)\n",
    "                            continue\n",
    "                    \n",
    "                # take argmin of cross-validation errors for each lag\n",
    "                cv_errors = np.mean(cv_errors, axis=1)\n",
    "                cv_lam = np.full((num_backcast), np.nan)\n",
    "                for k in range(num_backcast):\n",
    "                    cv_lam[k] = cv_grid[np.argmin(cv_errors[:, k])]\n",
    "                unique_lams = list(set(cv_lam))\n",
    "                \n",
    "                # perform final fits for each value of the tuning parameter set\n",
    "                out_values = np.full((num_backcast,), np.nan)\n",
    "                for lam in unique_lams:\n",
    "                    beta = fit_func(response, covariates, lam)\n",
    "                    est = (covariates @ beta).flatten()\n",
    "                    dates = training_dates\n",
    "                    if test_covariates is not None and test_dates is not None:\n",
    "                        est = np.r_[est, \n",
    "                                    (np.c_[np.ones(test_covariates.shape[0]), \n",
    "                                           test_covariates[:, ~nan_cols]] @ beta).flatten()]\n",
    "                        dates = np.r_[dates, test_dates]\n",
    "                        \n",
    "                    est = est[-num_backcast:]\n",
    "                    dates = dates[-num_backcast:]\n",
    "                    for k in range(num_backcast):\n",
    "                        if cv_lam[k] == lam:\n",
    "                            out_values[k] = est[k]\n",
    "                            \n",
    "                assert np.any(~np.isnan(out_values)), out_values\n",
    "                output[geo] = LocationSeries(geo, get_geo_type(geo), dict(zip(dates, out_values)))\n",
    "        directory.maybe_write_file(output, output_file, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443cdaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_grid = np.r_[np.logspace(1e-4, 1, 10) - 1, np.logspace(1.05, 2, 5)]\n",
    "cv_folds = 7\n",
    "plt.plot(cv_grid, marker=\".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b801e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_regularized_regression(\n",
    "    ridge_config, as_of_range, _ridge_regression,\n",
    "    full_sensors, infections_data,\n",
    "    full_sensor_data, cv_grid, cv_folds, directory, False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad17d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_regularized_regression(\n",
    "    fast_ridge_config, as_of_range, _ridge_regression,\n",
    "    fast_sensors, infections_data,\n",
    "    fast_sensor_data, cv_grid, cv_folds, directory, False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8b238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_regularized_regression(\n",
    "    lasso_config, as_of_range, _lasso_regression,\n",
    "    full_sensors, infections_data,\n",
    "    full_sensor_data, cv_grid, cv_folds, directory, False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b76163",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_regularized_regression(\n",
    "    fast_lasso_config, as_of_range, _lasso_regression,\n",
    "    fast_sensors, infections_data,\n",
    "    fast_sensor_data, cv_grid, cv_folds, directory, False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c45361",
   "metadata": {},
   "source": [
    "## Kalman filter fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7eb4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load population dataframe\n",
    "pop_df = pickle.load(open(\"./fusion/top_250_pops\", \"rb\"))\n",
    "pop_df.set_index(\"fips\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statespace(state_id, input_location_types, atom_list, pop_df=pop_df):\n",
    "    \"\"\"Specific to a state-only heirarchy.\"\"\"\n",
    "    \n",
    "    # list of all locations: state, county\n",
    "    all_location_types = [(state_id, 'state')]\n",
    "    for loc in atom_list:\n",
    "        all_location_types.append((loc, 'county'))\n",
    "    \n",
    "    def get_weight_row(location, location_type, atoms):\n",
    "        total_population = 0\n",
    "        atom_populations = []\n",
    "\n",
    "        if location_type == 'county':\n",
    "            for atom in atoms:\n",
    "                if atom == location:\n",
    "                    population = pop_df.loc[atom].population\n",
    "                else:\n",
    "                    population = 0\n",
    "                total_population += population\n",
    "                atom_populations.append(population)\n",
    "                \n",
    "        elif location_type == 'state':\n",
    "            for atom in atoms:\n",
    "                population = pop_df.loc[atom].population\n",
    "                total_population += population\n",
    "                atom_populations.append(population)\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"get_weight_row: invalid location_type passed\")\n",
    "\n",
    "        # sanity check\n",
    "        if total_population == 0:\n",
    "            raise Exception(('location has no constituent atoms', location))\n",
    "\n",
    "        get_fraction = lambda pop: pop / total_population\n",
    "        return list(map(get_fraction, atom_populations))\n",
    "\n",
    "    def get_weight_matrix(location_types, atoms):\n",
    "        \"\"\"Construct weight matrix.\"\"\"\n",
    "        get_row = lambda loc: get_weight_row(loc[0], loc[1], atoms)\n",
    "        return np.array(list(map(get_row, location_types)))\n",
    "\n",
    "    H0 = get_weight_matrix(input_location_types, atom_list)\n",
    "    W0 = get_weight_matrix(all_location_types, atom_list)\n",
    "    \n",
    "    # get H and W from H0 and W0\n",
    "    H, W, output_idx = fusion.determine_statespace(H0, W0)\n",
    "    output_locations = [all_location_types[i] for i in output_idx]\n",
    "\n",
    "    return H, W, output_locations\n",
    "\n",
    "\n",
    "def response_matrix(as_of, dates, input_geos, response_data):\n",
    "    n, p = len(dates), len(input_geos)\n",
    "    matrix = np.full((n, p), np.nan)\n",
    "    for i, date in enumerate(dates):\n",
    "        for j, geo in enumerate(input_geos):\n",
    "            matrix[i, j] = response_data.get_val(as_of, date, geo)\n",
    "    return matrix\n",
    "    \n",
    "def covariate_matrix(as_of, dates, input_pairs, covariate_data,\n",
    "                     return_nan_cols=True):\n",
    "    n, p = len(dates), len(input_pairs)\n",
    "    matrix = np.full((n, p), np.nan)\n",
    "    for i, date in enumerate(dates):\n",
    "        for j, (covariate, geo) in enumerate(input_pairs):\n",
    "            matrix[i, j] = covariate_data.get_val(as_of, date, covariate, geo)   \n",
    "    if return_nan_cols:\n",
    "        nan_cols = np.all(np.isnan(matrix), axis=0)\n",
    "        return matrix, nan_cols\n",
    "    return matrix\n",
    "\n",
    "def gen_kf_sf(\n",
    "    output_config, as_of_range, sensor_dict, \n",
    "    infections_data, sensor_data,\n",
    "    cv_grid, cv_folds, directory, overwrite\n",
    "):\n",
    "    \n",
    "    d = Config.max_delay_days\n",
    "    input_sensors = sorted(sensor_dict.keys())\n",
    "    for as_of in tqdm(as_of_range[8:]):\n",
    "        output_file = directory.sensor_file(output_config, as_of)\n",
    "        if directory.exists(output_file, overwrite):\n",
    "            print(output_file, 'exists')\n",
    "            continue\n",
    "            \n",
    "        # find last observed data according to given lags\n",
    "        last_infection_date = as_of - timedelta(directory.gt.lag)\n",
    "        last_date = as_of - timedelta(output_config.lag)\n",
    "        last_training_date = min(last_infection_date, last_date)\n",
    "        \n",
    "        # use 2d training window, determine if out-of-sample estimates can be made (n_test)\n",
    "        training_dates = [d.date() for d in date_range(as_of - timedelta(2*d), last_training_date)]\n",
    "        n_train = len(training_dates)\n",
    "        n_test = (last_date - last_infection_date).days\n",
    "\n",
    "        # train one model per state\n",
    "        output = {}\n",
    "        for state in states:\n",
    "            input_locations = state_map[state]\n",
    "            atom_list = [l for l in input_locations if l != state]\n",
    "                \n",
    "            # find sensors available at time-date pairs\n",
    "            input_pairs = np.array(list(itertools.product(input_sensors, input_locations)))\n",
    "            covariates, nan_cols = covariate_matrix(as_of, training_dates, input_pairs, sensor_data)\n",
    "            test_covariates, test_dates = None, None\n",
    "            if n_test >= 0:\n",
    "                test_dates = [d.date() for d in date_range(\n",
    "                    last_date - timedelta(n_test), last_date)][1:]\n",
    "                test_covariates, test_nan_cols = covariate_matrix(\n",
    "                    as_of, test_dates, input_pairs, sensor_data\n",
    "                )\n",
    "                nan_cols = np.logical_or(nan_cols, test_nan_cols)\n",
    "\n",
    "            input_pairs = input_pairs[~nan_cols]\n",
    "            input_geos = [l for s, l in input_pairs]\n",
    "            input_response = response_matrix(as_of, training_dates, input_geos, infections_data)\n",
    "            col_means = np.nanmean(covariates[:, ~nan_cols], axis=0)\n",
    "            covariates = mean_impute(covariates[:, ~nan_cols])\n",
    "            p = len(input_pairs)\n",
    "            \n",
    "            # determine measurement maps\n",
    "            H, W, output_locations = generate_statespace(\n",
    "                state, [(l, get_geo_type(l)) for s, l in input_pairs], atom_list)\n",
    "            output_geos = [l[0] for l in output_locations]\n",
    "            output_response = response_matrix(as_of, training_dates, output_geos, infections_data)\n",
    "\n",
    "            # perform cross-validation to find regularization parameter\n",
    "            cv_errors = np.full((len(cv_grid), cv_folds, num_backcast), np.inf)\n",
    "            for j, fold in enumerate(range(1, cv_folds + 1)):\n",
    "                n = covariates.shape[0] - fold\n",
    "                cv_as_of = as_of - timedelta(fold)\n",
    "                cv_training_dates = training_dates[:n]\n",
    "                \n",
    "                cv_covariates, cv_nan_cols = covariate_matrix(\n",
    "                    cv_as_of, cv_training_dates, input_pairs, sensor_data)\n",
    "                cv_input_response = response_matrix(\n",
    "                    cv_as_of, cv_training_dates, input_geos, infections_data)\n",
    "                cv_output_response = response_matrix(\n",
    "                    as_of, cv_training_dates, output_geos, infections_data)\n",
    "                \n",
    "                cv_col_means = col_means\n",
    "                if not np.any(cv_nan_cols):\n",
    "                    cv_col_means = np.nanmean(cv_covariates, axis=0)\n",
    "                cv_covariates = mean_impute(cv_covariates, cv_col_means)\n",
    "                \n",
    "                cv_test_covariates, cv_test_output_response = None, None\n",
    "                if n_test >= 0: # produce a 1-ahead prediction                    \n",
    "                    cv_test_covariates, _ = covariate_matrix(\n",
    "                        cv_as_of, [training_dates[n]], input_pairs, sensor_data)\n",
    "                    cv_test_covariates = mean_impute(cv_test_covariates, cv_col_means)\n",
    "                    cv_test_output_response = response_matrix(\n",
    "                        as_of, [training_dates[n]], output_geos, infections_data)\n",
    "                    cv_output_response = np.r_[\n",
    "                        cv_output_response, cv_test_output_response]\n",
    "                    \n",
    "                cv_noise = cv_input_response - cv_covariates\n",
    "                a, b = covariance.nancov(cv_noise)\n",
    "                R = a / b\n",
    "                for i, lam in enumerate(cv_grid):\n",
    "                    R_tilde = (1 - lam) * R + lam * np.eye(p)\n",
    "                    try:\n",
    "                        RiH = np.dot(np.linalg.inv(R_tilde), H)\n",
    "                        P = np.linalg.inv(np.dot(H.T, RiH))\n",
    "                        B = np.dot(P, RiH.T)\n",
    "                        cv_predictions = np.dot(W, np.dot(B, cv_covariates.T)).T\n",
    "                        assert cv_predictions.shape[0] == n\n",
    "                \n",
    "                        if n_test >= 0:\n",
    "                            cv_predictions = np.r_[\n",
    "                                cv_predictions,\n",
    "                                np.dot(W, np.dot(B, cv_test_covariates.T)).T.reshape(1, -1)]\n",
    "                        \n",
    "                        cv_errors[i, j, :] = np.mean(np.abs(cv_predictions - cv_output_response), axis=1)[-num_backcast:]\n",
    "                    except np.linalg.LinAlgError as e:\n",
    "                        print(geo, as_of, lam, e)\n",
    "                        print(np.sum(np.isnan(cv_covariates)))\n",
    "                        print(np.sum(np.isnan(cv_input_response)))\n",
    "                        print(np.sum(np.isnan(cv_noise)))\n",
    "                        print(R)\n",
    "                        print(R_tilde)\n",
    "                        continue\n",
    "\n",
    "            # take argmin of cross-validation errors for each lag\n",
    "            cv_errors = np.mean(cv_errors, axis=1)\n",
    "            cv_lam = np.full((num_backcast), np.nan)\n",
    "            for k in range(num_backcast):\n",
    "                cv_lam[k] = cv_grid[np.argmin(cv_errors[:, k])]\n",
    "            unique_lams = list(set(cv_lam))\n",
    "           \n",
    "            # perform final fits for each value of the tuning parameter set\n",
    "            out_values = np.full((len(output_locations), num_backcast), np.nan)\n",
    "            noise = input_response - covariates\n",
    "            a, b = covariance.nancov(noise)\n",
    "            R = a / b\n",
    "            for lam in unique_lams:\n",
    "                R_tilde = (1 - lam) * R + lam * np.eye(p)\n",
    "                RiH = np.linalg.inv(R_tilde) @ H\n",
    "                P = np.linalg.inv(H.T @ RiH)\n",
    "                B = P @ RiH.T\n",
    "                est = np.dot(W, B @ covariates.T)\n",
    "                dates = training_dates\n",
    "                if test_covariates is not None and test_dates is not None:\n",
    "                    est = np.c_[est, \n",
    "                        np.dot(W, np.dot(B, test_covariates[:, ~nan_cols].T))\n",
    "                    ]\n",
    "                    dates = np.r_[dates, test_dates]\n",
    "                \n",
    "                assert est.shape[1] == len(dates)\n",
    "                est = est[:, -num_backcast:]\n",
    "                dates = dates[-num_backcast:]\n",
    "                for k in range(num_backcast):\n",
    "                    if cv_lam[k] == lam:\n",
    "                        out_values[:, k] = est[:, k]\n",
    "            assert np.any(~np.isnan(out_values)), out_values\n",
    "                \n",
    "            # store output in dictionary\n",
    "            for i, (geo, geotype) in enumerate(output_locations):\n",
    "                # skip any geographies we are not interested in (megacounties)\n",
    "                if geo not in evaluation_geos:\n",
    "                    continue\n",
    "                output[geo] = LocationSeries(geo, geotype, dict(zip(dates, out_values[i, :])))\n",
    "        directory.maybe_write_file(output, output_file, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_sf_cv_grid = np.r_[1/(1+np.exp(-np.linspace(-10, 10, 30))), 1]\n",
    "cv_folds = 7\n",
    "\n",
    "plt.plot(kf_sf_cv_grid, marker=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c1d98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_kf_sf(\n",
    "    kf_sf_config, as_of_range,\n",
    "    full_sensors, infections_data,\n",
    "    full_sensor_data, kf_sf_cv_grid, cv_folds, directory, False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed588aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kf_sf(\n",
    "    fast_kf_sf_config, as_of_range,\n",
    "    fast_sensors, infections_data,\n",
    "    fast_sensor_data, kf_sf_cv_grid, cv_folds, directory, False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}